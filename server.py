
# streamlit run server.py
import streamlit as st 
# from skimage import io
# from skimage.transform import resize
import numpy as np  
import torch
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import torch
import torch. nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import warnings
from sklearn.model_selection import train_test_split
from streamlit_echarts import st_echarts
import torch 
import torch.nn as nn 
import torch.optim as optim 
import numpy as np 
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm 
import matplotlib.pyplot as plt 
import random 
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import train_test_split
from collections import Counter
from torchtext.vocab import vocab
import re
from bs4 import BeautifulSoup
import requests
import pandas as pd
from torchtext.data.utils import get_tokenizer
import os
import pandas as pd
import torch
from collections import Counter
import streamlit as st
import pyautogui
import joblib
from datetime import datetime

# æ¨¡å‹è¼‰å…¥
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
@st.cache_resource 

def load_model_1():#lstm_1
    
    class LSTM_1(nn.Module):
        
        def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
            super(LSTM_1, self).__init__()
            self.hidden_dim = hidden_dim
            self.num_layers = num_layers
            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  
            self.fc = nn.Linear(hidden_dim, output_dim)
            self.dropout = nn.Dropout(0.1) 

        def forward(self, x):
        
            batch_size = x.size(0)
            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)
            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)
            out, _ = self.lstm(x, (h0, c0)) 
            out = self.dropout(out)
            out = self.fc(out[:, -1, :])
            return out
        
    input_dim = 7
    hidden_dim =64
    num_layers = 2
    output_dim = 1
    weightpath='LSTM.pth'
    state_dict = torch.load(weightpath, map_location=device)
    model_1 = LSTM_1(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
    model_1.load_state_dict(state_dict) 
    model_1 = model_1.to(device)
    model_1.eval()
    return model_1

model_1 = load_model_1()
one_time=0



@st.cache_resource
def load_and_process_data(file_path, min_freq=15):
    """
    åŠ è¼‰æ•¸æ“šä¸¦é€²è¡Œåˆ†è©å’Œè©å½™è¡¨å»ºç«‹ã€‚
    æ­¤å‡½æ•¸åƒ…é‹è¡Œä¸€æ¬¡ï¼Œè¿”å›åˆ†è©å™¨å’Œè©å½™è¡¨ã€‚
    """
    # åŠ è¼‰ CSV æ–‡ä»¶
    df = pd.read_csv(file_path)
    reviews = df['review'].values
    sentiments = df['sentiment'].values

    # åˆå§‹åŒ–åˆ†è©å™¨
    tokenizer = get_tokenizer('basic_english')

    # è¨ˆæ•¸å–®è©é »ç‡
    counter = Counter()
    for review in reviews:
        token = tokenizer(review)
        counter.update(token)

    # å»ºç«‹è©å½™è¡¨
    token_vocab = vocab(counter, min_freq=min_freq, specials=('<pad>', '<unk>'))
    token_vocab.set_default_index(token_vocab.get_stoi()['<unk>'])

    return tokenizer, token_vocab, reviews, sentiments


# åŠ è¼‰æ•¸æ“šå’Œè©å½™è¡¨
file_path = 'IMDB_Dataset.csv'
tokenizer, token_vocab, reviews, sentiments = load_and_process_data(file_path)
PAD_IDX = token_vocab.get_stoi()['<pad>']
INPUT_DIM = len(token_vocab)








if "dialog_open" not in st.session_state:
    st.session_state.dialog_open = True  # åˆå§‹ç‹€æ…‹ç‚ºé–‹å•Ÿ

@st.dialog("è«‹åŒæ„ä»¥ä¸‹å…è²¬è²æ˜åŠä½¿ç”¨è¦ç¯„")
def disclaimer_dialog():
    """
    é¡¯ç¤ºå…è²¬è²æ˜çš„å°è©±æ¡†
    """
    st.markdown("""
            ## å…è²¬è²æ˜

            1. **é æ¸¬çµæœåƒ…ä¾›åƒè€ƒ**  
            æœ¬ç¶²ç«™æä¾›çš„æ‰€æœ‰è‚¡ç¥¨é æ¸¬çµæœã€æ•¸æ“šåˆ†æåŠå»ºè­°å‡åƒ…ä¾›åƒè€ƒã€‚é æ¸¬çµæœåŸºæ–¼æ­·å²æ•¸æ“šåŠAIæ¨¡å‹åˆ†æï¼Œä½†ç„¡æ³•ä¿è­‰å…¶æº–ç¢ºæ€§æˆ–æœªä¾†è¡¨ç¾ã€‚ä½¿ç”¨è€…æ‡‰è‡ªè¡Œåˆ¤æ–·å…¶æ±ºç­–ä¸¦æ‰¿æ“”é¢¨éšªã€‚

            2. **ä¸ä½œç‚ºæŠ•è³‡å»ºè­°**  
            æœ¬ç¶²ç«™æ‰€æä¾›çš„è³‡è¨Šã€é æ¸¬åŠåˆ†æå…§å®¹ä¸æ‡‰è¦–ç‚ºä»»ä½•å½¢å¼çš„æŠ•è³‡å»ºè­°ã€‚ç”¨æˆ¶åœ¨åšå‡ºæŠ•è³‡æ±ºç­–å‰ï¼Œæ‡‰è«®è©¢å°ˆæ¥­æŠ•è³‡é¡§å•ï¼Œä¸¦å……åˆ†äº†è§£æ‰€æœ‰é¢¨éšªã€‚

            3. **é¢¨éšªæç¤º**  
            è‚¡ç¥¨å¸‚å ´æ³¢å‹•æ€§å¤§ï¼ŒæŠ•è³‡æ¶‰åŠé¢¨éšªã€‚éå»çš„è¡¨ç¾ä¸ä»£è¡¨æœªä¾†çµæœï¼Œæ‰€æœ‰çš„æŠ•è³‡æ±ºç­–å’Œè¡Œç‚ºå‡ç”±ç”¨æˆ¶è‡ªè¡Œæ‰¿æ“”é¢¨éšªã€‚ç¶²ç«™ä¸å°ä»»ä½•å› ä½¿ç”¨æœ¬ç¶²ç«™è³‡è¨Šè€Œç”¢ç”Ÿçš„æå¤±æˆ–æå®³è² è²¬ã€‚

            4. **æ•¸æ“šæº–ç¢ºæ€§**  
            æœ¬ç¶²ç«™æä¾›çš„è³‡æ–™ä¾†è‡ªç¬¬ä¸‰æ–¹æ•¸æ“šæºã€‚é›–ç„¶æˆ‘å€‘å·²ç«­ç›¡å…¨åŠ›ç¢ºä¿è³‡æ–™çš„æº–ç¢ºæ€§èˆ‡æ™‚æ•ˆæ€§ï¼Œä½†ç„¡æ³•ä¿è­‰æ‰€æœ‰è³‡æ–™ç„¡èª¤æˆ–ç„¡å»¶é²ã€‚æœ¬ç¶²ç«™å°æ–¼å› æ•¸æ“šä¸æº–ç¢ºæˆ–å»¶é²é€ æˆçš„ä»»ä½•æå¤±ä¸æ‰¿æ“”è²¬ä»»ã€‚
            
            ## ä½¿ç”¨è¦ç¯„

            1. **ä½¿ç”¨è€…è²¬ä»»**  
            ç”¨æˆ¶åœ¨ä½¿ç”¨æœ¬ç¶²ç«™æœå‹™æ™‚ï¼Œæ‡‰ä¿è­‰æä¾›çš„æ‰€æœ‰è³‡è¨ŠçœŸå¯¦ã€æº–ç¢ºä¸”åˆæ³•ã€‚ç”¨æˆ¶ä¸å¾—åˆ©ç”¨æœ¬ç¶²ç«™é€²è¡Œä»»ä½•éæ³•æ´»å‹•ï¼ŒåŒ…æ‹¬ä½†ä¸é™æ–¼æ¬ºè©ã€æ“ç¸±å¸‚å ´æˆ–é•åç›¸é—œæ³•å¾‹æ³•è¦ã€‚

            2. **ç¦æ­¢å•†æ¥­ç”¨é€”**  
            æœ¬ç¶²ç«™åƒ…ä¾›å€‹äººä½¿ç”¨ï¼Œä¸å¾—ç”¨æ–¼ä»»ä½•å•†æ¥­ç”¨é€”ã€‚ç”¨æˆ¶ä¸å¾—å°‡ç¶²ç«™çš„å…§å®¹æˆ–æ•¸æ“šé€²è¡Œè½‰å”®ã€è¤‡è£½ã€ä¿®æ”¹æˆ–é‡æ–°ç™¼å¸ƒã€‚

            3. **ç‰ˆæ¬Šè²æ˜**  
            æœ¬ç¶²ç«™æ‰€æœ‰å…§å®¹ï¼ˆåŒ…æ‹¬ä½†ä¸é™æ–¼æ–‡æœ¬ã€åœ–åƒã€æ•¸æ“šåŠé æ¸¬çµæœï¼‰å‡å—ç‰ˆæ¬Šä¿è­·ã€‚æœªç¶“ç¶²ç«™æ˜ç¢ºæˆæ¬Šï¼Œä»»ä½•äººä¸å¾—ä»¥ä»»ä½•å½¢å¼ä½¿ç”¨ã€é‡è£½æˆ–åˆ†ç™¼æœ¬ç¶²ç«™çš„å…§å®¹ã€‚

            4. **éš±ç§æ”¿ç­–**  
            æˆ‘å€‘é‡è¦–ç”¨æˆ¶éš±ç§ï¼Œä¸¦æ‰¿è«¾æŒ‰ç…§éš±ç§æ”¿ç­–ä¿è­·ç”¨æˆ¶çš„å€‹äººè³‡è¨Šã€‚è©³ç´°éš±ç§æ”¿ç­–è«‹åƒè¦‹ç¶²ç«™çš„éš±ç§æ”¿ç­–é é¢ã€‚

            5. **æœå‹™ä¸­æ–·èˆ‡ä¿®æ”¹**  
            æœ¬ç¶²ç«™ä¿ç•™éš¨æ™‚ä¸­æ–·ã€ä¿®æ”¹æˆ–æ›´æ–°æœå‹™å…§å®¹çš„æ¬Šåˆ©ï¼Œä¸”ä¸éœ€äº‹å…ˆé€šçŸ¥ã€‚å°æ–¼ç”±æ–¼æœå‹™ä¸­æ–·æˆ–ä¿®æ”¹æ‰€å°è‡´çš„ä»»ä½•æå¤±ï¼Œæˆ‘å€‘ä¸æ‰¿æ“”è²¬ä»»ã€‚

            6. **è²¬ä»»é™åˆ¶**  
            åœ¨ä»»ä½•æƒ…æ³ä¸‹ï¼Œæœ¬ç¶²ç«™å°æ–¼ä»»ä½•å› ä½¿ç”¨ç¶²ç«™æœå‹™è€Œç”¢ç”Ÿçš„ç›´æ¥ã€é–“æ¥ã€å¶ç„¶ã€ç‰¹æ®Šæˆ–æ‡²ç½°æ€§æå®³æ¦‚ä¸è² è²¬ã€‚ç”¨æˆ¶ä½¿ç”¨æœ¬ç¶²ç«™æœå‹™çš„é¢¨éšªç”±ç”¨æˆ¶è‡ªè¡Œæ‰¿æ“”ã€‚
    """)
    if st.button("æˆ‘å·²é–±è®€ä¸¦åŒæ„"):
        st.session_state.dialog_open = False  # æ¨™è¨˜å°è©±æ¡†æ‡‰é—œé–‰
        st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥ç§»é™¤å°è©±æ¡†

# ä¸»é‚è¼¯
if st.session_state.dialog_open:
    disclaimer_dialog()






def predict_sentiment(text):
    # è‡ªå‹•è½‰ç¾©å–®å¼•è™Ÿ


    # æ­¥é©Ÿ 1ï¼šåˆ†è©å’Œç·¨ç¢¼
    tokens = tokenizer(text)  # ä½¿ç”¨åˆ†è©å™¨å°‡æ–‡æœ¬åˆ†è©
    indices = token_vocab.lookup_indices(tokens)  # å°‡åˆ†è©çµæœè½‰æ›ç‚ºæ•¸å­—ç´¢å¼•
    text_tensor = torch.tensor(indices).unsqueeze(0).to(device)  # å°‡æ•¸å­—ç´¢å¼•è½‰æ›ç‚ºTensorä¸¦åŠ å…¥batchç¶­åº¦

    # æ­¥é©Ÿ 2ï¼šä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬
    model_2.eval()  # è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è¨ˆç®—
        output = model_2(text_tensor)  # æ¨¡å‹é æ¸¬
        prediction = torch.sigmoid(output).item()  # å–å‡ºé æ¸¬çµæœä¸¦æ‡‰ç”¨sigmoidå‡½æ•¸
    pred = (output.view(-1) > 0.5)
    return label_decoding[int(pred)]
    #print('Pred Label:',label_decoding[int(pred)])             # é¡¯ç¤ºæ–‡å­— 

    # ç¤ºä¾‹è¼¸å…¥

def load_model_2():#lstm_2
    
    class IMDB(Dataset):
        def __init__(self, x, y):
            self.x = x
            self.y = y
            
        def __getitem__(self, index):
            return self.x[index], self.y[index]
        
        def __len__(self):
            return len(self.x)

    class LSTM_2(nn.Module):
        def __init__(self, embedding_dim, hidden_size, num_layers=1, bidirectional=True):
            super().__init__()
            self.embedding = nn.Embedding(INPUT_DIM,  embedding_dim, padding_idx = PAD_IDX)
            self.lstm =nn.LSTM(embedding_dim, 
                            hidden_size = hidden_size, 
                            num_layers = num_layers,
                            bidirectional = bidirectional,
                            batch_first=True
            )

            hidden = hidden_size * 2 if bidirectional else hidden_size
            self.fc = nn.Linear(hidden, 1)

            self.sigmoid = nn.Sigmoid()

        def forward(self, x):
            emb_out = self.embedding(x)
            out, (h, c)  = self.lstm(emb_out)
            x = out[:, -1, :]
            x = self.fc(x)
            print(x.shape)
            return self.sigmoid(x)
        
        
    model_2 = LSTM_2(embedding_dim = 300, hidden_size= 512).to(device)

    model_path='mode5l.pt'

    model_2.load_state_dict(torch.load(model_path, map_location=device))
    model_2.eval()
    return model_2

model_2 = load_model_2()

def cnn2dm():
    timestep = 20  # æ™‚é–“æ­¥é•·
    feature_size = 5  # ç‰¹å¾µæ•¸é‡
    out_channels = [16, 32, 64]  # å·ç©è¼¸å‡ºé€šé“
    output_size = 1  # è¼¸å‡ºå¤§å°
    class CNN2D(nn.Module):
        def __init__(self, feature_size, timestep, out_channels, output_size):
            super(CNN2D, self).__init__()

            # å®šä¹‰äºŒç»´å·ç§¯å±‚
            self.conv2d_1 = nn.Conv2d(1, out_channels[0], kernel_size=3, padding=1)
            self.conv2d_2 = nn.Conv2d(out_channels[0], out_channels[1], kernel_size=3, padding=1)
            self.conv2d_3 = nn.Conv2d(out_channels[1], out_channels[2], kernel_size=3, padding=1)

            # Pooling layer to reduce dimensions
            self.pool = nn.MaxPool2d(kernel_size=2, stride=1)

            # Calculate output size after convolution and pooling
            self.conv_output_size = self.calculate_conv_output_size(timestep, feature_size)

            # Define fully connected layers
            self.fc_1 = nn.Linear(896, 256)
            self.fc_2 = nn.Linear(256, output_size)

            # Activation function
            self.relu = nn.ReLU()

        def calculate_conv_output_size(self, timestep, feature_size):
            def calc_dim(input_size, kernel_size, stride, padding):
                return (input_size - kernel_size + 2 * padding) // stride + 1

            # First convolution layer
            height = calc_dim(timestep, 3, 1, 1)
            width = calc_dim(feature_size, 3, 1, 1)

            # First pooling layer
            height = calc_dim(height, 2, 2, 0)
            width = calc_dim(width, 2, 2, 0)

            # Second convolution layer
            height = calc_dim(height, 3, 1, 1)
            width = calc_dim(width, 3, 1, 1)

            # Second pooling layer
            height = calc_dim(height, 2, 2, 0)
            width = calc_dim(width, 2, 2, 0)

            # Third convolution layer
            height = calc_dim(height, 3, 1, 1)
            width = calc_dim(width, 3, 1, 1)

            # Third pooling layer
            height = calc_dim(height, 2, 2, 0)
            width = calc_dim(width, 2, 2, 0)

            return height, width
        def forward(self, x):
            #print(f"Input shape: {x.shape}")  # Debug
            x = self.pool(self.relu(self.conv2d_1(x)))
            #print(f"After conv2d_1: {x.shape}")  # Debug
            x = self.pool(self.relu(self.conv2d_2(x)))
            #print(f"After conv2d_2: {x.shape}")  # Debug
            x = self.pool(self.relu(self.conv2d_3(x)))
            #print(f"After conv2d_3: {x.shape}")  # Debug
            x = x.view(x.size(0), -1)
            #print(f"Flattened shape: {x.shape}")  # Debug
            x = self.relu(self.fc_1(x))
            x = self.fc_2(x)
            return x
    
    model_3 = CNN2D(feature_size=feature_size, timestep=timestep, out_channels=out_channels, output_size=output_size)
    model_3.load_state_dict(torch.load("2dcnn.pth"))
    timestep = 10  # æ—¶é—´æ­¥ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
    model_3.eval()  # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    return model_3

model_dcnn = cnn2dm()

    

with st.sidebar:
    st.header("è¯çµ¡è³‡è¨Š")
    st.write("å¦‚æœæ‚¨æœ‰ä»»ä½•å•é¡Œï¼Œè«‹éš¨æ™‚è¯ç¹«æˆ‘å€‘ï¼")

    # è¯çµ¡è³‡è¨Š
    st.subheader("è¯çµ¡æ–¹å¼")
    st.write("é›»å­éƒµä»¶: 411123002@gms.ndhu.edu.tw")
    st.write("é›»è©±: 0800-000-050")
    
    st.write("---")
    
    # å…¶ä»–å´é‚Šæ¬„å…§å®¹
    st.subheader("ç¶²ç«™å°èˆª")
    st.write("[é¦–é ](https://vskuzygkniaubvpqqbzoot.streamlit.app/)")
    st.write("[Github](https://github.com/kkk-source)")
    st.write("[FAQ](https://github.com/kkk-source/lstm/issues/1)")
    st.write("[éš±ç§æ”¿ç­–](/privacy_policy)")
    
    st.write("---")
    
    # é¡¯ç¤ºç‰ˆæ¬Šè³‡è¨Š
    st.text("Â© 2024 ä¿ç•™æ‰€æœ‰æ¬Šåˆ©ã€‚")


st.title("å°ç£åŠ æ¬ŠæŒ‡æ•¸(^TWII)")
st.caption("å‚™è¨»ï¼šæ•¸æ“šä¾†æºç‚º Yahoo Financeï¼Œæ›´æ–°é–“éš”ç‚º 1 åˆ†é˜ã€‚ä¸”ä¸åŒ…å«ç›¤å¾Œæ•¸æ“šã€‚")

import yfinance as yf
import streamlit as st
stock_code = "^TWII"
stock = yf.Ticker(stock_code)
data_today = stock.history(period="1d", interval="1m")  
data_recent = stock.history(period="5d", interval="1d") #
if not data_today.empty and len(data_recent) >= 2:
    # ä»Šæ—¥é–‹ç›¤åƒ¹ã€æœ€é«˜åƒ¹ã€æœ€ä½åƒ¹
    today_open = data_today["Close"].iloc[-1]
    today_high = data_today["High"].max()
    today_low = data_today["Low"].min()  
    
    # å‰ä¸€å¤©æ”¶ç›¤åƒ¹
    yesterday_close = data_recent["Close"].iloc[-2]
    yesterday_h = data_today["High"].iloc[-2]
    yesterday_l = data_today["Low"].iloc[-2]
    # è¨ˆç®—å·®ç•°
    open_diff = today_open - yesterday_close
    high_diff = today_high - yesterday_h
    low_diff = today_low - yesterday_l
    open_diff = today_open - yesterday_close
    high_diff = today_high - yesterday_h
    low_diff = today_low - yesterday_l
    
    open_pct = (open_diff / yesterday_close) * 100
    high_pct = (high_diff / yesterday_h) * 100
    low_pct = (low_diff / yesterday_l) * 100
    col31, col32, col33 = st.columns(3)
    col31.metric("æ™‚å¯¦æ•¸æ“š", f"{today_open:.2f}", f"{open_diff:+.2f} ({open_pct:+.2f}%)")
    col32.metric("ä»Šæ—¥æœ€é«˜", f"{today_high:.2f}", f"{high_diff:+.2f} ({high_pct:+.2f}%)")
    col33.metric("ä»Šæ—¥æœ€ä½", f"{today_low:.2f}", f"{low_diff:+.2f} ({low_pct:+.2f}%)")
else:
    st.error("ç„¡æ³•ç²å–å®Œæ•´çš„æ•¸æ“šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")



import streamlit as st
import re
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
import streamlit as st

def csv_content():
    """çˆ¬å– PTT è‚¡ç¥¨æ¿æœ€è¿‘ 60 ç¯‡æœ‰æ•ˆæ–‡ç« ä¸¦å„²å­˜ç‚º CSV"""
    base_url = "https://www.ptt.cc/bbs/Stock/index.html"
    posts = []
    min_length = 50  # æœ€å°å…§æ–‡é•·åº¦é™åˆ¶
    target_count = 60  # ç›®æ¨™æ–‡ç« æ•¸é‡
    progress = 0  # åˆå§‹é€²åº¦

    # Streamlit é€²åº¦æ¢
    progress_bar = st.progress(progress)
    status_text = st.empty()

    while len(posts) < target_count:
        response = requests.get(base_url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # ç²å–ç¬¦åˆæ¢ä»¶çš„æ–‡ç« 
            new_posts = get_ptt_posts(soup, min_length)
            posts.extend(new_posts)
            
            # æ›´æ–°é€²åº¦æ¢
            progress = min(len(posts) / target_count, 1.0)  # ç¢ºä¿é€²åº¦ä¸è¶…é 100%
            progress_bar.progress(progress)
            status_text.text(f"å·²æŠ“å–æ–‡ç« æ•¸é‡ï¼š{len(posts)}/{target_count}")
            deletedcontain=len(posts)-target_count

            # å¦‚æœç¸½æ•¸å·²é”ç›®æ¨™æ•¸é‡ï¼Œå‰‡çµæŸ
            if len(posts) >= target_count:
                posts = posts[:target_count]  # åªä¿ç•™ç›®æ¨™æ•¸é‡çš„æ–‡ç« 
                break

            # æ‰¾åˆ°ä¸Šä¸€é çš„é€£çµ
            prev_link_tag = soup.find("a", class_="btn wide", string="â€¹ ä¸Šé ")
            if prev_link_tag:
                prev_link = "https://www.ptt.cc" + prev_link_tag["href"]
                base_url = prev_link
            else:
                st.error("ç„¡æ³•æ‰¾åˆ°ä¸Šä¸€é ï¼Œåœæ­¢çˆ¬å–")
                break
        else:
            st.error(f"ç„¡æ³•å–å¾—ç¶²é å…§å®¹ï¼ŒHTTP ç‹€æ…‹ç¢¼ï¼š{response.status_code}")
            break

        time.sleep(1)  # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚

    # åŒ¯å‡ºè³‡æ–™
    if posts:
        df = pd.DataFrame(posts)
        df.to_csv(r"ptt_stock_filtered_content.csv", index=False, encoding="utf-8-sig")
        

    else:
        st.error("æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–‡ç« ")
    return len(posts),deletedcontain

def get_ptt_posts(soup, min_length):
    """å¾ PTT é é¢è§£æç¬¦åˆæ¢ä»¶çš„æ–‡ç« """
    data = soup.select("div.r-ent")
    result = []
    for item in data:
        try:
            # æŠ“å–æ–‡ç« æ¨™é¡Œ
            title = item.select_one("div.title").text.strip()
            
            # æŠ“å–æ–‡ç« é€£çµ
            link_tag = item.select_one("div.title a")
            if link_tag:
                article_link = "https://www.ptt.cc" + link_tag["href"]
                article_response = requests.get(article_link)
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.text, 'html.parser')
                    # æŠ“å–æ–‡ç« å®Œæ•´å…§å®¹
                    full_content = article_soup.select_one("div#main-content").text.strip()
                    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æ­£æ–‡å…§å®¹
                    content_match = re.search(r"æ™‚é–“.*?\n(.*?)(?:--|$)", full_content, re.DOTALL)
                    content = content_match.group(1).strip() if content_match else "ç„¡æ³•æå–å…§æ–‡"
                else:
                    content = "ç„¡æ³•å–å¾—å…§å®¹"
            else:
                content = "ç„¡æ³•å–å¾—å…§å®¹"
            
            # å¦‚æœå…§æ–‡é•·åº¦å°æ–¼æŒ‡å®šæœ€å°å­—æ•¸ï¼Œå‰‡è·³é
            if len(content) >= min_length:
                result.append({"title": title, "content": content})
        except Exception as e:
            print(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            continue
    return result

# --------------------------------------------------------------------------------------
st.write("---")
st.title("å¸‚å ´æƒ…æ„Ÿåˆ¤æ–·")

with st.container():
    col1, col2 = st.columns(2)

    with col1:
        clicked1 = st.button("æ›´æ–°æ–‡ç« ", help="æ›´æ–°æœ€è¿‘ 60 ç¯‡æœ‰æ•ˆæ–‡ç« ")
        if clicked1:
            len_post,deletedcontain=csv_content()
    
    with col2:
        clicked2 = st.button("æ›´æ–°æƒ…æ„Ÿçµ±è¨ˆ", help="æ›´æ–°æœ€è¿‘ 60 ç¯‡æœ‰æ•ˆæ–‡ç« æƒ…æ„Ÿçµ±è¨ˆ")
        if clicked2:
            len_post,deletedcontain=csv_content()
colsuccess, colwarning = st.columns(2)
if clicked1:
    with colsuccess:
        st.success(f"è³‡æ–™å·²æˆåŠŸåŒ¯å‡ºé»æ“Šæ›´æ–°æƒ…æ„Ÿä»¥æ›´æ–°æœ‰æ•ˆæ–‡ç« æƒ…æ„Ÿçµ±è¨ˆï¼š{len_post}")
    with colwarning:
        st.warning(f"å·²åˆªé™¤{deletedcontain}ç¯‡ä¸åˆè¦æ±‚ä¹‹æ–‡ç« ")


# --------------------------------------------------------------------------------------
st.write("---")
timestep = 10


# åŠ è½½Scaler
sc = joblib.load("sc.pkl")
sc_2 = joblib.load("sc_2.pkl")

ticker_all = ["2303.TW", "2330.TW", "2317.TW", "2412.TW", "3008.TW"]
pre_all = [0, 0, 0, 0, 0]

# å¾ªç¯éå†æ‰€æœ‰è‚¡ç¥¨
i = 0
end_date = datetime.today().strftime('%Y-%m-%d')
start_date = '2024-11-28'

for ticker_ever in ticker_all:
  
    data = yf.download(ticker_ever, start=start_date, end=end_date)
    data.reset_index(inplace=True)
    
    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„ç‰¹å¾åˆ—
    data_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    data = data[data_columns].values.astype('float')
    
    # å½’ä¸€åŒ–æ•°æ®
    data = sc.transform(data) 
    

    
    # åˆ›å»ºæ—¶é—´æ­¥è¾“å…¥æ•°æ®
    def prepare_prediction_data(data, timestep):
        input_data = []
        for j in range(len(data) - timestep):
            input_data.append(data[j:j + timestep])
        return np.array(input_data)
    
    input_data = prepare_prediction_data(data, timestep)
    input_tensor = torch.from_numpy(input_data).to(torch.float32)
    
    # æ¨¡å‹é¢„æµ‹
    with torch.no_grad():
        predictions = model_dcnn(input_tensor.unsqueeze(1))  # æ·»åŠ é€šé“ç»´åº¦
    
    # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ
    predictions = predictions.numpy()
    predictions_inverse = sc_2.inverse_transform(predictions)
    
    # ä¿å­˜å½“å‰è‚¡ç¥¨çš„é¢„æµ‹ç»“æœ
    pre_all[i] = round(predictions_inverse[-1].item(), 2)
    i=i+1


with st.container():
    coldcnn1, coldcnn2,coldcnn3,coldcnn4,coldcnn5 = st.columns(5)
    i=0
    for ticker_ever in ticker_all:
        if i == 0:
            coldcnn1.metric(label=ticker_ever, value=f"{pre_all[i]}")  # æ˜¾ç¤ºç»“æœ
        elif i == 1:
            coldcnn2.metric(label=ticker_ever, value=f"{pre_all[i]}")
        elif i == 2:
            coldcnn3.metric(label=ticker_ever, value=f"{pre_all[i]}")
        elif i == 3:
            coldcnn4.metric(label=ticker_ever, value=f"{pre_all[i]}")
        else:
            coldcnn5.metric(label=ticker_ever, value=f"{pre_all[i]}")
        
        i += 1

    




# --------------------------------------------------------------------------------------
st.write("---")
uploaded_file = st.file_uploader("æ¸¬è©¦")

if uploaded_file is not None:
    ee=[]
    try:
        def transform_data(df):

            data_index =  ['Close','Volume2','h-l','sma','10ema','greed','adr']
        
            flatten_data = df[data_index].values.reshape(-1)  # æ”¤å¹³è³‡æ–™
            
            # è½‰æ›æˆå­—ä¸²
            str_data = "<SEP>".join(flatten_data.astype('str'))
            filter_data = str_data.replace(',',"").replace('X',"")
            
            # åˆ‡å‰²å›é™£åˆ—
            x_data = filter_data.split("<SEP>")
            
            return x_data
            
        x = []
        df = pd.read_csv(uploaded_file)
        df2=df['Close']
        data = transform_data(df)

        data_index=['Close']
        data2 = df[data_index].values.reshape(-1)  # æ”¤å¹³è³‡æ–™
        ee.extend(data2)

        x.extend(data)
        sc = MinMaxScaler()
        sc_2 = MinMaxScaler()


        x = np.array(x).astype('float')
        x = x.reshape(-1, len(['Close','Volume2','h-l','sma','10ema','greed','adr']))

        ee = np.array(ee).astype('float')
        ee = ee.reshape(-1, 1)
        ee=sc_2.fit_transform(ee)

        x = sc.fit_transform(x)
        y = x[:,0]


        def split_data(datas, labels, split_num = 10):
            max_len = len(datas)
            x, y = [], []
            for i in range(max_len - split_num -1):
                x.append(datas[i: i+split_num])
                y.append(labels[split_num+i+1])
            
            return np.array(x), np.array(y)


        x_train, x_valid, y_train, y_valid = train_test_split(x, y, train_size=1, shuffle=False)

        x_valid, y_valid = split_data(x_valid, y_valid)

        x_valid=torch.from_numpy(x_valid).type(torch.Tensor)
        y_valid=torch.from_numpy(y_valid).float().unsqueeze(1)
        x_valid = x_valid
        y_valid=y_valid

        with torch.no_grad():
            x_valid = x_valid.to(device)  # Move x_valid to the same device as the model
            y_pred = model_1(x_valid)
        y_pred = y_pred.cpu().numpy()
        y_valid = y_valid.cpu().numpy()
        y_pred_inverse = sc_2.inverse_transform(y_pred)
        y_valid_inverse = sc_2.inverse_transform(y_valid)
        # print(y_pred_inverse.item())#é æ¸¬
        # print(y_valid_inverse.item())#åŸå§‹
        

        def display_predictions(y_pred_inverse, y_valid_inverse):
            template = f"""
            ### é æ¸¬çµæœé æ¸¬:{round(y_pred_inverse.item(),1)}

            

            ### é æ¸¬çµæœåŸå§‹:{y_valid_inverse.item()}
            
            """
            st.markdown(template)
            st.toast('ä¸Šå‚³æˆåŠŸ!!', icon='ğŸ‰')

        display_predictions(y_pred_inverse, y_valid_inverse)
    except Exception as e:
         st.error(f"æ–‡ä»¶è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤è«‹ç¢ºèªä¸Šå‚³æ–‡ä»¶æ ¼å¼")
    
# Store the initial value of widgets in session state
if "visibility" not in st.session_state:
    st.session_state.visibility = "visible"
    st.session_state.disabled = False


text_input = st.text_input(
    "Enter some text ğŸ‘‡",
    placeholder="è¼¸å…¥æ–‡ç« ",
    label_visibility="visible",
    disabled=False,

)



if text_input:
    from deep_translator import GoogleTranslator
    label_decoding = {0:'negative', 1:'positive'}
   

    user_input = text_input  # å‡è¨­ text_input æ˜¯ç”¨æˆ¶è¼¸å…¥çš„æ–‡æœ¬
    
    # ä½¿ç”¨ deep-translator é€²è¡Œç¿»è­¯ï¼Œå¾ç¹é«”ä¸­æ–‡ç¿»è­¯åˆ°è‹±æ–‡
    translation = GoogleTranslator(source='zh-TW', target='en').translate(user_input)
    user_input = translation  # æ›´æ–°ç‚ºç¿»è­¯å¾Œçš„æ–‡æœ¬

    
    tokenizer = get_tokenizer('basic_english')
    ans=predict_sentiment(user_input)
    if(ans=='positive'):{
        st.balloons()
    }
    else:
        st.snow()
    st.text_area("è½‰ç‚ºè‹±æ–‡ï¼š", user_input, height=200)
    
    st.write("è¼¸å…¥æ–‡ç« æƒ…ç·’ï¼š", ans)
    

