
# streamlit run server.py
import streamlit as st 
# from skimage import io
# from skimage.transform import resize
import numpy as np  
import torch
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import torch
import torch. nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import warnings
from sklearn.model_selection import train_test_split
from streamlit_echarts import st_echarts
import torch 
import torch.nn as nn 
import torch.optim as optim 
import numpy as np 
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm 
import matplotlib.pyplot as plt 
import random 
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import train_test_split
from collections import Counter
from torchtext.vocab import vocab
import re
from bs4 import BeautifulSoup
import requests
import pandas as pd
from torchtext.data.utils import get_tokenizer
import os
import pandas as pd
import torch
from collections import Counter
import streamlit as st
import joblib
from datetime import datetime
from deep_translator import GoogleTranslator

if "dialog_open" not in st.session_state:
    st.session_state.dialog_open = True  # åˆå§‹ç‹€æ…‹ç‚ºé–‹å•Ÿ

@st.dialog("è«‹åŒæ„ä»¥ä¸‹å…è²¬è²æ˜åŠä½¿ç”¨è¦ç¯„")
def disclaimer_dialog():
    """
    é¡¯ç¤ºå…è²¬è²æ˜çš„å°è©±æ¡†
    """
    st.markdown("""
            ## å…è²¬è²æ˜

            1. **é æ¸¬çµæœåƒ…ä¾›åƒè€ƒ**  
            æœ¬ç¶²ç«™æä¾›çš„æ‰€æœ‰è‚¡ç¥¨é æ¸¬çµæœã€æ•¸æ“šåˆ†æåŠå»ºè­°å‡åƒ…ä¾›åƒè€ƒã€‚é æ¸¬çµæœåŸºæ–¼æ­·å²æ•¸æ“šåŠAIæ¨¡å‹åˆ†æï¼Œä½†ç„¡æ³•ä¿è­‰å…¶æº–ç¢ºæ€§æˆ–æœªä¾†è¡¨ç¾ã€‚ä½¿ç”¨è€…æ‡‰è‡ªè¡Œåˆ¤æ–·å…¶æ±ºç­–ä¸¦æ‰¿æ“”é¢¨éšªã€‚

            2. **ä¸ä½œç‚ºæŠ•è³‡å»ºè­°**  
            æœ¬ç¶²ç«™æ‰€æä¾›çš„è³‡è¨Šã€é æ¸¬åŠåˆ†æå…§å®¹ä¸æ‡‰è¦–ç‚ºä»»ä½•å½¢å¼çš„æŠ•è³‡å»ºè­°ã€‚ç”¨æˆ¶åœ¨åšå‡ºæŠ•è³‡æ±ºç­–å‰ï¼Œæ‡‰è«®è©¢å°ˆæ¥­æŠ•è³‡é¡§å•ï¼Œä¸¦å……åˆ†äº†è§£æ‰€æœ‰é¢¨éšªã€‚

            3. **é¢¨éšªæç¤º**  
            è‚¡ç¥¨å¸‚å ´æ³¢å‹•æ€§å¤§ï¼ŒæŠ•è³‡æ¶‰åŠé¢¨éšªã€‚éå»çš„è¡¨ç¾ä¸ä»£è¡¨æœªä¾†çµæœï¼Œæ‰€æœ‰çš„æŠ•è³‡æ±ºç­–å’Œè¡Œç‚ºå‡ç”±ç”¨æˆ¶è‡ªè¡Œæ‰¿æ“”é¢¨éšªã€‚ç¶²ç«™ä¸å°ä»»ä½•å› ä½¿ç”¨æœ¬ç¶²ç«™è³‡è¨Šè€Œç”¢ç”Ÿçš„æå¤±æˆ–æå®³è² è²¬ã€‚

            4. **æ•¸æ“šæº–ç¢ºæ€§**  
            æœ¬ç¶²ç«™æä¾›çš„è³‡æ–™ä¾†è‡ªç¬¬ä¸‰æ–¹æ•¸æ“šæºã€‚é›–ç„¶æˆ‘å€‘å·²ç«­ç›¡å…¨åŠ›ç¢ºä¿è³‡æ–™çš„æº–ç¢ºæ€§èˆ‡æ™‚æ•ˆæ€§ï¼Œä½†ç„¡æ³•ä¿è­‰æ‰€æœ‰è³‡æ–™ç„¡èª¤æˆ–ç„¡å»¶é²ã€‚æœ¬ç¶²ç«™å°æ–¼å› æ•¸æ“šä¸æº–ç¢ºæˆ–å»¶é²é€ æˆçš„ä»»ä½•æå¤±ä¸æ‰¿æ“”è²¬ä»»ã€‚
            
            ## ä½¿ç”¨è¦ç¯„

            1. **ä½¿ç”¨è€…è²¬ä»»**  
            ç”¨æˆ¶åœ¨ä½¿ç”¨æœ¬ç¶²ç«™æœå‹™æ™‚ï¼Œæ‡‰ä¿è­‰æä¾›çš„æ‰€æœ‰è³‡è¨ŠçœŸå¯¦ã€æº–ç¢ºä¸”åˆæ³•ã€‚ç”¨æˆ¶ä¸å¾—åˆ©ç”¨æœ¬ç¶²ç«™é€²è¡Œä»»ä½•éæ³•æ´»å‹•ï¼ŒåŒ…æ‹¬ä½†ä¸é™æ–¼æ¬ºè©ã€æ“ç¸±å¸‚å ´æˆ–é•åç›¸é—œæ³•å¾‹æ³•è¦ã€‚

            2. **ç¦æ­¢å•†æ¥­ç”¨é€”**  
            æœ¬ç¶²ç«™åƒ…ä¾›å€‹äººä½¿ç”¨ï¼Œä¸å¾—ç”¨æ–¼ä»»ä½•å•†æ¥­ç”¨é€”ã€‚ç”¨æˆ¶ä¸å¾—å°‡ç¶²ç«™çš„å…§å®¹æˆ–æ•¸æ“šé€²è¡Œè½‰å”®ã€è¤‡è£½ã€ä¿®æ”¹æˆ–é‡æ–°ç™¼å¸ƒã€‚

            3. **ç‰ˆæ¬Šè²æ˜**  
            æœ¬ç¶²ç«™æ‰€æœ‰å…§å®¹ï¼ˆåŒ…æ‹¬ä½†ä¸é™æ–¼æ–‡æœ¬ã€åœ–åƒã€æ•¸æ“šåŠé æ¸¬çµæœï¼‰å‡å—ç‰ˆæ¬Šä¿è­·ã€‚æœªç¶“ç¶²ç«™æ˜ç¢ºæˆæ¬Šï¼Œä»»ä½•äººä¸å¾—ä»¥ä»»ä½•å½¢å¼ä½¿ç”¨ã€é‡è£½æˆ–åˆ†ç™¼æœ¬ç¶²ç«™çš„å…§å®¹ã€‚

            4. **éš±ç§æ”¿ç­–**  
            æˆ‘å€‘é‡è¦–ç”¨æˆ¶éš±ç§ï¼Œä¸¦æ‰¿è«¾æŒ‰ç…§éš±ç§æ”¿ç­–ä¿è­·ç”¨æˆ¶çš„å€‹äººè³‡è¨Šã€‚è©³ç´°éš±ç§æ”¿ç­–è«‹åƒè¦‹ç¶²ç«™çš„éš±ç§æ”¿ç­–é é¢ã€‚

            5. **æœå‹™ä¸­æ–·èˆ‡ä¿®æ”¹**  
            æœ¬ç¶²ç«™ä¿ç•™éš¨æ™‚ä¸­æ–·ã€ä¿®æ”¹æˆ–æ›´æ–°æœå‹™å…§å®¹çš„æ¬Šåˆ©ï¼Œä¸”ä¸éœ€äº‹å…ˆé€šçŸ¥ã€‚å°æ–¼ç”±æ–¼æœå‹™ä¸­æ–·æˆ–ä¿®æ”¹æ‰€å°è‡´çš„ä»»ä½•æå¤±ï¼Œæˆ‘å€‘ä¸æ‰¿æ“”è²¬ä»»ã€‚

            6. **è²¬ä»»é™åˆ¶**  
            åœ¨ä»»ä½•æƒ…æ³ä¸‹ï¼Œæœ¬ç¶²ç«™å°æ–¼ä»»ä½•å› ä½¿ç”¨ç¶²ç«™æœå‹™è€Œç”¢ç”Ÿçš„ç›´æ¥ã€é–“æ¥ã€å¶ç„¶ã€ç‰¹æ®Šæˆ–æ‡²ç½°æ€§æå®³æ¦‚ä¸è² è²¬ã€‚ç”¨æˆ¶ä½¿ç”¨æœ¬ç¶²ç«™æœå‹™çš„é¢¨éšªç”±ç”¨æˆ¶è‡ªè¡Œæ‰¿æ“”ã€‚
    """)
    if st.button("æˆ‘å·²é–±è®€ä¸¦åŒæ„"):
        st.session_state.dialog_open = False  # æ¨™è¨˜å°è©±æ¡†æ‡‰é—œé–‰
        st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥ç§»é™¤å°è©±æ¡†


    
# æ¨¡å‹è¼‰å…¥
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
@st.cache_resource 
def load_model_1():#lstm_1
    
    class LSTM_1(nn.Module):
        
        def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
            super(LSTM_1, self).__init__()
            self.hidden_dim = hidden_dim
            self.num_layers = num_layers
            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  
            self.fc = nn.Linear(hidden_dim, output_dim)
            self.dropout = nn.Dropout(0.1) 

        def forward(self, x):
        
            batch_size = x.size(0)
            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)
            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)
            out, _ = self.lstm(x, (h0, c0)) 
            out = self.dropout(out)
            out = self.fc(out[:, -1, :])
            return out
        
    input_dim = 5
    hidden_dim =64
    num_layers = 2
    output_dim = 1
    weightpath='LSTM.pth'
    state_dict = torch.load(weightpath, map_location=device)
    model_1 = LSTM_1(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
    model_1.load_state_dict(state_dict) 
    model_1 = model_1.to(device)
    model_1.eval()
    return model_1

model_1 = load_model_1()
one_time=0



@st.cache_resource
def load_and_process_data(file_path, min_freq=15):
    """
    åŠ è¼‰æ•¸æ“šä¸¦é€²è¡Œåˆ†è©å’Œè©å½™è¡¨å»ºç«‹ã€‚
    æ­¤å‡½æ•¸åƒ…é‹è¡Œä¸€æ¬¡ï¼Œè¿”å›åˆ†è©å™¨å’Œè©å½™è¡¨ã€‚
    """
    # åŠ è¼‰ CSV æ–‡ä»¶
    df = pd.read_csv(file_path)
    reviews = df['review'].values
    sentiments = df['sentiment'].values

    # åˆå§‹åŒ–åˆ†è©å™¨
    tokenizer = get_tokenizer('basic_english')

    # è¨ˆæ•¸å–®è©é »ç‡
    counter = Counter()
    for review in reviews:
        token = tokenizer(review)
        counter.update(token)

    # å»ºç«‹è©å½™è¡¨
    token_vocab = vocab(counter, min_freq=min_freq, specials=('<pad>', '<unk>'))
    token_vocab.set_default_index(token_vocab.get_stoi()['<unk>'])

    return tokenizer, token_vocab, reviews, sentiments


# åŠ è¼‰æ•¸æ“šå’Œè©å½™è¡¨
file_path = 'IMDB_Dataset.csv'
tokenizer, token_vocab, reviews, sentiments = load_and_process_data(file_path)
PAD_IDX = token_vocab.get_stoi()['<pad>']
INPUT_DIM = len(token_vocab)


label_decoding = {0:'negative', 1:'positive'}
def predict_sentiment(text):
    # è‡ªå‹•è½‰ç¾©å–®å¼•è™Ÿ


    # æ­¥é©Ÿ 1ï¼šåˆ†è©å’Œç·¨ç¢¼
    tokens = tokenizer(text)  # ä½¿ç”¨åˆ†è©å™¨å°‡æ–‡æœ¬åˆ†è©
    indices = token_vocab.lookup_indices(tokens)  # å°‡åˆ†è©çµæœè½‰æ›ç‚ºæ•¸å­—ç´¢å¼•
    text_tensor = torch.tensor(indices).unsqueeze(0).to(device)  # å°‡æ•¸å­—ç´¢å¼•è½‰æ›ç‚ºTensorä¸¦åŠ å…¥batchç¶­åº¦

    # æ­¥é©Ÿ 2ï¼šä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬
    model_2.eval()  # è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è¨ˆç®—
        output = model_2(text_tensor)  # æ¨¡å‹é æ¸¬
        prediction = torch.sigmoid(output).item()  # å–å‡ºé æ¸¬çµæœä¸¦æ‡‰ç”¨sigmoidå‡½æ•¸
    pred = (output.view(-1) > 0.5)
    return label_decoding[int(pred)]
    #print('Pred Label:',label_decoding[int(pred)])             # é¡¯ç¤ºæ–‡å­— 

    # ç¤ºä¾‹è¼¸å…¥

def load_model_2():#lstm_2
    
    class IMDB(Dataset):
        def __init__(self, x, y):
            self.x = x
            self.y = y
            
        def __getitem__(self, index):
            return self.x[index], self.y[index]
        
        def __len__(self):
            return len(self.x)

    class LSTM_2(nn.Module):
        def __init__(self, embedding_dim, hidden_size, num_layers=1, bidirectional=True):
            super().__init__()
            self.embedding = nn.Embedding(INPUT_DIM,  embedding_dim, padding_idx = PAD_IDX)
            self.lstm =nn.LSTM(embedding_dim, 
                            hidden_size = hidden_size, 
                            num_layers = num_layers,
                            bidirectional = bidirectional,
                            batch_first=True
            )

            hidden = hidden_size * 2 if bidirectional else hidden_size
            self.fc = nn.Linear(hidden, 1)

            self.sigmoid = nn.Sigmoid()

        def forward(self, x):
            emb_out = self.embedding(x)
            out, (h, c)  = self.lstm(emb_out)
            x = out[:, -1, :]
            x = self.fc(x)
            print(x.shape)
            return self.sigmoid(x)
        
        
    model_2 = LSTM_2(embedding_dim = 300, hidden_size= 512).to(device)

    model_path='mode5l.pt'

    model_2.load_state_dict(torch.load(model_path, map_location=device))
    model_2.eval()
    return model_2

model_2 = load_model_2()

def cnn2dm():
    timestep = 20  # æ™‚é–“æ­¥é•·
    feature_size = 5  # ç‰¹å¾µæ•¸é‡
    out_channels = [16, 32, 64]  # å·ç©è¼¸å‡ºé€šé“
    output_size = 1  # è¼¸å‡ºå¤§å°
    class CNN2D(nn.Module):
        def __init__(self, feature_size, timestep, out_channels, output_size):
            super(CNN2D, self).__init__()

            # å®šä¹‰äºŒç»´å·ç§¯å±‚
            self.conv2d_1 = nn.Conv2d(1, out_channels[0], kernel_size=3, padding=1)
            self.conv2d_2 = nn.Conv2d(out_channels[0], out_channels[1], kernel_size=3, padding=1)
            self.conv2d_3 = nn.Conv2d(out_channels[1], out_channels[2], kernel_size=3, padding=1)

            # Pooling layer to reduce dimensions
            self.pool = nn.MaxPool2d(kernel_size=2, stride=1)

            # Calculate output size after convolution and pooling
            self.conv_output_size = self.calculate_conv_output_size(timestep, feature_size)

            # Define fully connected layers
            self.fc_1 = nn.Linear(896, 256)
            self.fc_2 = nn.Linear(256, output_size)

            # Activation function
            self.relu = nn.ReLU()

        def calculate_conv_output_size(self, timestep, feature_size):
            def calc_dim(input_size, kernel_size, stride, padding):
                return (input_size - kernel_size + 2 * padding) // stride + 1

            # First convolution layer
            height = calc_dim(timestep, 3, 1, 1)
            width = calc_dim(feature_size, 3, 1, 1)

            # First pooling layer
            height = calc_dim(height, 2, 2, 0)
            width = calc_dim(width, 2, 2, 0)

            # Second convolution layer
            height = calc_dim(height, 3, 1, 1)
            width = calc_dim(width, 3, 1, 1)

            # Second pooling layer
            height = calc_dim(height, 2, 2, 0)
            width = calc_dim(width, 2, 2, 0)

            # Third convolution layer
            height = calc_dim(height, 3, 1, 1)
            width = calc_dim(width, 3, 1, 1)

            # Third pooling layer
            height = calc_dim(height, 2, 2, 0)
            width = calc_dim(width, 2, 2, 0)

            return height, width
        def forward(self, x):
            #print(f"Input shape: {x.shape}")  # Debug
            x = self.pool(self.relu(self.conv2d_1(x)))
            #print(f"After conv2d_1: {x.shape}")  # Debug
            x = self.pool(self.relu(self.conv2d_2(x)))
            #print(f"After conv2d_2: {x.shape}")  # Debug
            x = self.pool(self.relu(self.conv2d_3(x)))
            #print(f"After conv2d_3: {x.shape}")  # Debug
            x = x.view(x.size(0), -1)
            #print(f"Flattened shape: {x.shape}")  # Debug
            x = self.relu(self.fc_1(x))
            x = self.fc_2(x)
            return x
    
    model_3 = CNN2D(feature_size=feature_size, timestep=timestep, out_channels=out_channels, output_size=output_size)
    model_3.load_state_dict(torch.load("2dcnn.pth"))
    timestep = 10  # æ—¶é—´æ­¥ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
    model_3.eval()  # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    return model_3

model_dcnn = cnn2dm()
# ä¸»é‚è¼¯
if st.session_state.dialog_open:
    disclaimer_dialog()
    


import google.generativeai as genai
import streamlit as st

# è¨­ç½®æ¨™é¡Œ
st.title("æœ‰å•é¡Œå—?å•å•Gemini")
st.caption("ğŸš€power by gemini")
with st.expander("å¦‚ä½•ç”³è«‹ Gemini API å¯†é‘°"):
    st.write("""
        è¦åœ¨ä½ çš„æ‡‰ç”¨ç¨‹å¼ä¸­ä½¿ç”¨ Gemini APIï¼Œä½ éœ€è¦ä¸€å€‹ API å¯†é‘°ã€‚è«‹ä¾ç…§ä»¥ä¸‹æ­¥é©Ÿç”³è«‹å¯†é‘°ï¼š

        1. **è¨»å†Š Google Cloud å¸³è™Ÿï¼š**
           - é€ è¨ª [Google Cloud å®˜æ–¹ç¶²ç«™](https://cloud.google.com/)ï¼Œä¸¦è¨»å†Šä¸€å€‹å¸³è™Ÿã€‚å¦‚æœä½ å·²ç¶“æœ‰å¸³è™Ÿï¼Œè«‹ç›´æ¥ç™»å…¥ã€‚

        2. **å•Ÿç”¨ Gemini APIï¼š**
           - ç™»å…¥å¾Œï¼Œå‰å¾€ [API æœå‹™é é¢](https://console.cloud.google.com/).
           - æœå°‹ **"Gemini"** ä¸¦å•Ÿç”¨ Gemini APIã€‚
           - é»æ“Š **"å•Ÿç”¨"** ä¾†é–‹é€šè©²æœå‹™ã€‚

        3. **å–å¾— API å¯†é‘°ï¼š**
           - åœ¨ API è¨­å®šé é¢ï¼Œé€²å…¥ **Credentials**ï¼ˆèªè­‰ï¼‰é¸é …ã€‚
           - é»æ“Š **"Create Credentials"**ï¼ˆå‰µå»ºèªè­‰ï¼‰ï¼Œé¸æ“‡ **API key**ã€‚
           - ç³»çµ±å°‡ç”Ÿæˆä¸€å€‹ API å¯†é‘°ï¼Œè¨˜å¾—è¤‡è£½ä¸¦å¦¥å–„ä¿å­˜ã€‚

        4. **å°‡ API å¯†é‘°æ·»åŠ åˆ°ä½ çš„æ‡‰ç”¨ç¨‹å¼ä¸­ï¼š**
           - ç¾åœ¨ä½ å¯ä»¥å°‡é€™å€‹ API å¯†é‘°è²¼å…¥åˆ°ä½ çš„æ‡‰ç”¨ç¨‹å¼ä¸­ï¼Œä¾‹å¦‚åœ¨ä½ çš„ Streamlit æ‡‰ç”¨ä¸­ã€‚
           - ç¢ºä¿å¯†é‘°ä»¥ **"AIza"** é–‹é ­ï¼ˆä¾‹å¦‚ï¼š"AXXXXXXXXXXXXXXX--6XXXXXXXXXXXXXXXXXX"ï¼‰ã€‚

        5. **å®‰å…¨æ€§æ³¨æ„äº‹é …ï¼š**
           - è«‹ä¿ç®¡å¥½ä½ çš„ API å¯†é‘°ï¼Œé¿å…å…¬é–‹åˆ†äº«æˆ–æš´éœ²åœ¨å®¢æˆ¶ç«¯ç¨‹å¼ç¢¼ä¸­ã€‚
           - å»ºè­°ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æˆ–å¯†é‘°ç®¡ç†å·¥å…·ä¾†æå‡å®‰å…¨æ€§ã€‚

        å–å¾— API å¯†é‘°å¾Œï¼Œä½ å°±å¯ä»¥å°‡å…¶æ•´åˆåˆ°ä½ çš„æ‡‰ç”¨ç¨‹å¼ä¸­ï¼Œä½¿ç”¨ Gemini API æä¾›çš„åŠŸèƒ½ä¾†ç”Ÿæˆå…§å®¹ã€‚
    """)
# Add custom CSS to adjust the position of the chat input box
st.markdown(
    """
    <style>
    .stTextInput > div {
        margin-bottom: 400px;  /* Adjust this value to move the input box up */
    }
    </style>
    """,
    unsafe_allow_html=True
)

api_key = "AIzaSyD2RxE2U6ZgJJHJHXcqOpXiG_--TQxvhCI"
genai.configure(api_key=api_key)
model_name = st.selectbox(
    "è«‹é¸æ“‡Geminiæ¨¡å‹",
    options=["gemini-2.0-flash-exp","gemini-2.0-flash-lite","gemini-2.0-flash-thinking-exp-01-21"],
    key="model_selector"
)

st.caption(f"ç•¶å‰é¸æ“‡çš„æ¨¡å‹æ˜¯: {model_name}")

model = genai.GenerativeModel(model_name)

# åˆå§‹åŒ– session_state
if "messages" not in st.session_state:
    st.session_state.messages = []

# é¡¯ç¤ºæ­·å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("LSTMæ˜¯ç”šéº¼? å•å•Gemini"):

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            response = model.generate_content(prompt)
            reply = response.text
            st.markdown(reply)
        except Exception as e:
            reply = f"Error: {e}"
            st.markdown(reply)

    st.session_state.messages.append({"role": "assistant", "content": reply})
    st.write("---")
with st.sidebar:
    st.header("è¯çµ¡è³‡è¨Š")
    st.write("å¦‚æœæ‚¨æœ‰ä»»ä½•å•é¡Œï¼Œè«‹éš¨æ™‚è¯ç¹«æˆ‘å€‘ï¼")

    # è¯çµ¡è³‡è¨Š
    st.subheader("è¯çµ¡æ–¹å¼")
    st.write("é›»å­éƒµä»¶: 411123002@gms.ndhu.edu.tw")

    
    st.write("---")
    
    # å…¶ä»–å´é‚Šæ¬„å…§å®¹
    st.subheader("ç¶²ç«™å°èˆª")
    st.write("[é¦–é ](https://stocksupporter.streamlit.app/)")
    st.write("[Github](https://github.com/kkk-source)")

    st.write("[éš±ç§æ”¿ç­–](/privacy_policy)")
    st.write("[æ›´æ–°æ—¥èªŒ](/log)")
    st.write("---")
    
    # é¡¯ç¤ºç‰ˆæ¬Šè³‡è¨Š
    st.text("Â© 2024 ä¿ç•™æ‰€æœ‰æ¬Šåˆ©ã€‚")


st.title("å°ç£åŠ æ¬ŠæŒ‡æ•¸(^TWII)")
st.caption("å‚™è¨»ï¼šæ•¸æ“šä¾†æºç‚º Yahoo Financeï¼Œæ›´æ–°é–“éš”ç‚º 1 åˆ†é˜ã€‚ä¸”ä¸åŒ…å«ç›¤å¾Œæ•¸æ“šã€‚")

import yfinance as yf
import streamlit as st
stock_code = "^TWII"
stock = yf.Ticker(stock_code)
data_today = stock.history(period="1d", interval="1m")  
data_recent = stock.history(period="5d", interval="1d") #
if not data_today.empty and len(data_recent) >= 2:
    # ä»Šæ—¥é–‹ç›¤åƒ¹ã€æœ€é«˜åƒ¹ã€æœ€ä½åƒ¹
    today_open = data_today["Close"].iloc[-1]
    today_high = data_today["High"].max()
    today_low = data_today["Low"].min()  
    
    # å‰ä¸€å¤©æ”¶ç›¤åƒ¹
    yesterday_close = data_recent["Close"].iloc[-2]
    yesterday_h = data_today["High"].iloc[-2]
    yesterday_l = data_today["Low"].iloc[-2]
    # è¨ˆç®—å·®ç•°
    open_diff = today_open - yesterday_close
    high_diff = today_high - yesterday_h
    low_diff = today_low - yesterday_l
    open_diff = today_open - yesterday_close
    high_diff = today_high - yesterday_h
    low_diff = today_low - yesterday_l
    
    open_pct = (open_diff / yesterday_close) * 100
    high_pct = (high_diff / yesterday_h) * 100
    low_pct = (low_diff / yesterday_l) * 100
    col31, col32, col33 = st.columns(3)
    col31.metric("æ™‚å¯¦æ•¸æ“š", f"{today_open:.2f}", f"{open_diff:+.2f} ({open_pct:+.2f}%)")
    col32.metric("ä»Šæ—¥æœ€é«˜", f"{today_high:.2f}", f"{high_diff:+.2f} ({high_pct:+.2f}%)")
    col33.metric("ä»Šæ—¥æœ€ä½", f"{today_low:.2f}", f"{low_diff:+.2f} ({low_pct:+.2f}%)")
else:
    st.error("ç„¡æ³•ç²å–å®Œæ•´çš„æ•¸æ“šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
lstm_intro = """
### ä»€éº¼æ˜¯ LSTMï¼ˆé•·çŸ­æœŸè¨˜æ†¶ï¼‰ï¼Ÿ

LSTMï¼ˆLong Short-Term Memoryï¼‰æ˜¯ä¸€ç¨®ç‰¹æ®Šçš„å¾ªç’°ç¥ç¶“ç¶²çµ¡ï¼ˆRNNï¼‰ï¼Œå®ƒèƒ½å¤ åœ¨é•·æ™‚é–“å…§ä¿æŒè¨˜æ†¶ï¼Œå¾è€Œè§£æ±ºå‚³çµ± RNN åœ¨è™•ç†é•·åºåˆ—æ™‚çš„æ¢¯åº¦æ¶ˆå¤±å•é¡Œã€‚

#### LSTM çš„çµæ§‹
LSTM ç”±ä»¥ä¸‹å¹¾å€‹ä¸»è¦éƒ¨åˆ†çµ„æˆï¼š

1. **éºå¿˜é–€ï¼ˆForget Gateï¼‰ï¼š**
   - æ±ºå®šå“ªäº›ä¿¡æ¯æ‡‰è©²è¢«ä¸Ÿæ£„ï¼Œå“ªäº›æ‡‰è©²ä¿ç•™ã€‚å®ƒæª¢æŸ¥å‰ä¸€ç‹€æ…‹çš„è¼¸å‡ºå’Œç•¶å‰çš„è¼¸å…¥ï¼Œä¸¦è¼¸å‡ºä¸€å€‹ 0 åˆ° 1 ä¹‹é–“çš„å€¼ï¼Œè¡¨ç¤ºæ‡‰ä¿ç•™çš„è¨˜æ†¶ã€‚

2. **è¼¸å…¥é–€ï¼ˆInput Gateï¼‰ï¼š**
   - æ±ºå®šç•¶å‰è¼¸å…¥æ‡‰è©²å°è¨˜æ†¶é€²è¡Œå¤šå°‘ä¿®æ”¹ã€‚å®ƒåŒ…å«å…©å€‹éƒ¨åˆ†ï¼šä¸€å€‹æ˜¯ç”¨ä¾†æ›´æ–°è¨˜æ†¶çš„å€™é¸å±¤ï¼Œå¦ä¸€å€‹æ˜¯æ§åˆ¶æœ‰å¤šå°‘å€™é¸è¨˜æ†¶æ‡‰è©²è¢«åŠ å…¥åˆ°å–®å…ƒç‹€æ…‹ä¸­çš„éƒ¨åˆ†ã€‚

3. **å–®å…ƒç‹€æ…‹ï¼ˆCell Stateï¼‰ï¼š**
   - å­˜å„²äº†éå»çš„é•·æœŸè¨˜æ†¶ï¼Œä¸¦æ ¹æ“šå¿˜è¨˜é–€å’Œè¼¸å…¥é–€çš„çµæœé€²è¡Œæ›´æ–°ã€‚é€™æ˜¯ LSTM çš„é—œéµéƒ¨åˆ†ã€‚

4. **è¼¸å‡ºé–€ï¼ˆOutput Gateï¼‰ï¼š**
   - æ ¹æ“šå–®å…ƒç‹€æ…‹å’Œç•¶å‰è¼¸å…¥ï¼Œæ±ºå®šè¼¸å‡ºå¤šå°‘ä¿¡æ¯åˆ°ä¸‹å€‹æ™‚é–“æ­¥ã€‚

#### LSTM çš„å„ªé»
- **è§£æ±ºæ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼š** ç›¸æ¯”æ–¼å‚³çµ±çš„ RNNï¼ŒLSTM å¯ä»¥æ•æ‰é•·æœŸä¾è³´ï¼Œä¸¦ä¸”èƒ½å¤ é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±å•é¡Œã€‚
- **æ™‚é–“åºåˆ—é æ¸¬ï¼š** LSTM ç‰¹åˆ¥é©åˆè™•ç†æ™‚é–“åºåˆ—æ•¸æ“šï¼Œæ¯”å¦‚èªéŸ³è­˜åˆ¥ã€èªè¨€å»ºæ¨¡ç­‰ã€‚

#### LSTM çš„æ‡‰ç”¨é ˜åŸŸ
- **èªéŸ³è­˜åˆ¥ï¼š** ç”¨æ–¼å°‡èªéŸ³ä¿¡è™Ÿè½‰æ›ç‚ºæ–‡å­—ã€‚
- **èªè¨€è™•ç†ï¼š** ç”¨æ–¼æ©Ÿå™¨ç¿»è­¯å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»å‹™ã€‚
- **é‡‘èé æ¸¬ï¼š** ç”¨æ–¼è‚¡ç¥¨åƒ¹æ ¼é æ¸¬ã€éŠ·å”®é æ¸¬ç­‰ã€‚

LSTM æ˜¯è™•ç†æ™‚é–“åºåˆ—æ•¸æ“šçš„å¼·å¤§å·¥å…·ï¼Œèƒ½å¤ è¨˜ä½é—œéµçš„æ™‚é–“æ­¥ä¿¡æ¯ï¼Œä¸¦å¿½ç•¥ä¸å¿…è¦çš„å™ªè²ã€‚
"""
cnn_intro = """
### ä»€éº¼æ˜¯ 2D CNNï¼ˆå·ç©ç¥ç¶“ç¶²çµ¡ï¼‰ï¼Ÿ

2D å·ç©ç¥ç¶“ç¶²çµ¡ï¼ˆ2D CNNï¼‰æ˜¯ä¸€ç¨®æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œä¸»è¦ç”¨æ–¼è™•ç†2Dæ•¸æ“šï¼ˆå¦‚åœ–åƒï¼‰ã€‚å®ƒé€šéå·ç©æ“ä½œå­¸ç¿’åœ–åƒä¸­çš„ç©ºé–“ç‰¹å¾µï¼Œä¸¦é€šéå¤šå±¤å †ç–Šä¾†æå–å¾ä½å±¤åˆ°é«˜å±¤çš„ç‰¹å¾µã€‚

#### 2D CNN çš„çµæ§‹
1. **å·ç©å±¤ï¼ˆConvolutional Layerï¼‰ï¼š**
   - ä½¿ç”¨æ¿¾æ³¢å™¨ï¼ˆæˆ–ç¨±ç‚ºå·ç©æ ¸ï¼‰ä¾†æƒæåœ–åƒï¼Œæå–åœ–åƒçš„å±€éƒ¨ç‰¹å¾µï¼ˆå¦‚é‚Šç·£ã€è§’è½ç­‰ï¼‰ã€‚é€™ä¸€å±¤é€šéå·ç©é‹ç®—ç”Ÿæˆç‰¹å¾µåœ–ï¼ˆfeature mapï¼‰ã€‚

2. **æ± åŒ–å±¤ï¼ˆPooling Layerï¼‰ï¼š**
   - ç”¨æ–¼ç¸®å°åœ–åƒçš„ç©ºé–“å°ºå¯¸ï¼Œå¾è€Œæ¸›å°‘è¨ˆç®—é‡ä¸¦é˜²æ­¢éæ“¬åˆã€‚æœ€å¸¸è¦‹çš„æ˜¯æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰å’Œå¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰ã€‚

3. **æ¿€æ´»å‡½æ•¸ï¼ˆActivation Functionï¼‰ï¼š**
   - é€šå¸¸ä½¿ç”¨ ReLUï¼ˆRectified Linear Unitï¼‰æ¿€æ´»å‡½æ•¸ï¼Œä¾†å¢åŠ éç·šæ€§ï¼Œä½¿ç¥ç¶“ç¶²çµ¡èƒ½å¤ å­¸ç¿’æ›´åŠ è¤‡é›œçš„æ¨¡å¼ã€‚

4. **å…¨é€£æ¥å±¤ï¼ˆFully Connected Layerï¼‰ï¼š**
   - åœ¨å·ç©å±¤å’Œæ± åŒ–å±¤ä¹‹å¾Œï¼Œå°‡å­¸åˆ°çš„ç‰¹å¾µé€²è¡Œåˆ†é¡æˆ–å›æ­¸ä»»å‹™ã€‚

5. **è¼¸å‡ºå±¤ï¼ˆOutput Layerï¼‰ï¼š**
   - æ ¹æ“šä»»å‹™çš„éœ€æ±‚ï¼Œè¼¸å‡ºä¸åŒçš„çµæœï¼Œåˆ†é¡ä»»å‹™é€šå¸¸ä½¿ç”¨ softmax å‡½æ•¸é€²è¡Œå¤šåˆ†é¡ï¼Œå›æ­¸ä»»å‹™å‰‡ç›´æ¥è¼¸å‡ºé€£çºŒå€¼ã€‚

#### 2D CNN çš„å„ªé»
- **ç©ºé–“ä¸è®Šæ€§ï¼š** CNN èƒ½å¤ å­¸ç¿’åœ–åƒä¸­çš„å±€éƒ¨ç‰¹å¾µï¼Œä¸¦å°åœ–åƒé€²è¡Œå¹³ç§»ã€æ—‹è½‰ç­‰è®Šæ›çš„åˆ¤æ–·ã€‚
- **åƒæ•¸å…±äº«ï¼š** å·ç©å±¤ä¸­çš„æ¿¾æ³¢å™¨æ˜¯å…±äº«çš„ï¼Œé€™æ„å‘³è‘—æ¯å€‹æ¿¾æ³¢å™¨åœ¨æ•´å€‹åœ–åƒä¸­éƒ½æ˜¯ç›¸åŒçš„ï¼Œé€™å¤§å¤§æ¸›å°‘äº†åƒæ•¸æ•¸é‡ã€‚
- **å±¤æ¬¡ç‰¹å¾µå­¸ç¿’ï¼š** é€šéå¤šå±¤å·ç©å’Œæ± åŒ–æ“ä½œï¼ŒCNN èƒ½å¤ å¾ä½å±¤åˆ°é«˜å±¤é€æ­¥å­¸ç¿’åœ–åƒä¸­çš„è¤‡é›œç‰¹å¾µã€‚

#### 2D CNN çš„æ‡‰ç”¨é ˜åŸŸ
- **åœ–åƒåˆ†é¡ï¼š** ç”¨æ–¼å°ä¸åŒé¡åˆ¥çš„åœ–åƒé€²è¡Œåˆ†é¡ï¼ˆä¾‹å¦‚ï¼Œè¾¨è­˜è²“ç‹—åœ–åƒï¼‰ã€‚
- **ç‰©é«”æª¢æ¸¬ï¼š** ç”¨æ–¼è­˜åˆ¥åœ–åƒä¸­çš„ç‰¹å®šç‰©é«”ä½ç½®ã€‚
- **é¢éƒ¨è­˜åˆ¥ï¼š** ç”¨æ–¼æª¢æ¸¬åœ–åƒä¸­çš„äººè‡‰ä¸¦é€²è¡Œè­˜åˆ¥ã€‚
- **é†«å­¸å½±åƒåˆ†æï¼š** ç”¨æ–¼åˆ†æé†«å­¸å½±åƒï¼ˆå¦‚ X å…‰ç‰‡ã€CT æ‰«æç­‰ï¼‰é€²è¡Œç–¾ç—…è¨ºæ–·ã€‚

2D CNN åœ¨åœ–åƒè™•ç†é ˜åŸŸå–å¾—äº†é¡¯è‘—çš„æˆåŠŸï¼Œä¸¦æˆç‚ºè¨±å¤šè¦–è¦ºè­˜åˆ¥ä»»å‹™çš„åŸºç¤ã€‚
"""


import streamlit as st
import re
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
import streamlit as st

def csv_content():
    base_url = "https://www.ptt.cc/bbs/Stock/index.html"
    posts = []
    min_length = 50  # æœ€å°å…§æ–‡é•·åº¦é™åˆ¶
    target_count = 60  # ç›®æ¨™æ–‡ç« æ•¸é‡
    progress = 0  # åˆå§‹é€²åº¦

    # Streamlit é€²åº¦æ¢
    progress_bar = st.progress(progress)
    status_text = st.empty()

    while len(posts) < target_count:
        response = requests.get(base_url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # ç²å–ç¬¦åˆæ¢ä»¶çš„æ–‡ç« 
            new_posts = get_ptt_posts(soup, min_length)
            posts.extend(new_posts)
            
            # æ›´æ–°é€²åº¦æ¢
            progress = min(len(posts) / target_count, 1.0)  # ç¢ºä¿é€²åº¦ä¸è¶…é 100%
            progress_bar.progress(progress)
            status_text.text(f"å·²æŠ“å–æ–‡ç« æ•¸é‡ï¼š{len(posts)}/{target_count}")
            deletedcontain=len(posts)-target_count
            

            # å¦‚æœç¸½æ•¸å·²é”ç›®æ¨™æ•¸é‡ï¼Œå‰‡çµæŸ
            if len(posts) >= target_count:
                posts = posts[:target_count]  # åªä¿ç•™ç›®æ¨™æ•¸é‡çš„æ–‡ç« 
                break

            # æ‰¾åˆ°ä¸Šä¸€é çš„é€£çµ
            prev_link_tag = soup.find("a", class_="btn wide", string="â€¹ ä¸Šé ")
            if prev_link_tag:
                prev_link = "https://www.ptt.cc" + prev_link_tag["href"]
                base_url = prev_link
            else:
                st.error("ç„¡æ³•æ‰¾åˆ°ä¸Šä¸€é ï¼Œåœæ­¢çˆ¬å–")
                break
        else:
            st.error(f"ç„¡æ³•å–å¾—å®Œæ•´ç¶²é å…§å®¹ï¼ŒHTTP ç‹€æ…‹ç¢¼ï¼š{response.status_code}")
            break

        time.sleep(0.5)  # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚

    # åŒ¯å‡ºè³‡æ–™
    if posts:
        df = pd.DataFrame(posts)
        df.to_csv(r"ptt_stock_filtered_content.csv", index=False, encoding="utf-8-sig")
        

    else:
        st.error("æ²’æœ‰æŠ“å–åˆ°ä»»ä½•æ–‡ç« ")
    return len(posts),deletedcontain

def get_ptt_posts(soup, min_length):
    """å¾ PTT é é¢è§£æç¬¦åˆæ¢ä»¶çš„æ–‡ç« """
    data = soup.select("div.r-ent")
    result = []
    for item in data:
        try:
            # æŠ“å–æ–‡ç« æ¨™é¡Œ
            title = item.select_one("div.title").text.strip()
            if "[å…¬å‘Š]" in title:
                continue
            # æŠ“å–æ–‡ç« é€£çµ
            link_tag = item.select_one("div.title a")
            if link_tag:
                article_link = "https://www.ptt.cc" + link_tag["href"]
                article_response = requests.get(article_link)
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.text, 'html.parser')
                    # æŠ“å–æ–‡ç« å®Œæ•´å…§å®¹
                    full_content = article_soup.select_one("div#main-content").text.strip()
                    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æ­£æ–‡å…§å®¹
                    content_match = re.search(r"æ™‚é–“.*?\n(.*?)(?:--|$)", full_content, re.DOTALL)
                    content = content_match.group(1).strip() if content_match else "ç„¡æ³•æå–å…§æ–‡"
                else:
                    content = "ç„¡æ³•å–å¾—å…§å®¹"
            else:
                content = "ç„¡æ³•å–å¾—å…§å®¹"
            
            # å¦‚æœå…§æ–‡é•·åº¦å°æ–¼æŒ‡å®šæœ€å°å­—æ•¸ï¼Œå‰‡è·³é
            if len(content) >= min_length and  len(content)<2000:
                result.append({"title": title, "content": content})
        except Exception as e:
            print(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            continue
    return result

def sen_ana(sentiment_counts):
    file_path=r'ptt_stock_filtered_content.csv'
    data = pd.read_csv(file_path)
    
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_items = len(data['content'])
    for index, content in enumerate(data['content']):
        if isinstance(content, str):

            content = content.replace("\r\n", " ").replace("\n", " ").replace("ã€€", " " ).replace("-", " ").strip()
            content = ' '.join(content.split())
            content = re.sub(r'[^\w\s]', '', content)
            content = content.lower()
            content = content.encode('utf-8', errors='ignore').decode('utf-8')
            #
            if(((index + 1) / total_items)==50):
                st.caption(content)
                st.caption(len(content))
            
            translation = GoogleTranslator(source='zh-TW', target='en').translate(content)

            sentiment_code = predict_sentiment(translation)
            

            
            sentiment_label = "positive" if sentiment_code == "positive" else "negative"

            sentiment_counts[sentiment_label] += 1
            
            progress_bar.progress((index + 1) / total_items)
            status_text.text(f"å·²å®Œæˆæ•¸é‡ï¼š{index + 1}/{60}")

            results.append({"Original": content, "Translated": translation, "Sentiment": sentiment_label})
            
    return results

def display_bar_chart(sentiment_counts):
    # é…ç½®ç›´æ–¹åœ–é¸é …
    option = {
        "backgroundColor": "#212121",
        "title": {
            "text": "æƒ…æ„Ÿåˆ†æçµ±è¨ˆ",
            "subtext": "è³‡æ–™ä¾†æº:è‚¡ç¥¨è«–å£‡",
            "x": "left",
            "textStyle": {
                "color": "#f2f2f2"
            }
        },
        "tooltip": {
            "trigger": "axis",
            "axisPointer": {
                "type": "shadow"
            }
        },
        "legend": {
            "data": ["COUNT"],
            "textStyle": {
                "color": "#f2f2f2"
            }
        },
        "xAxis": {
            "type": "category",
            "data": ["positive","negative"],
            "axisLine": {
                "lineStyle": {
                    "color": "#f2f2f2"
                }
            },
            "axisLabel": {
 
                "interval": 0 
            }
        },
        "yAxis": {
            "type": "value",
            "axisLine": {
                "lineStyle": {
                    "color": "#f2f2f2"
                }
            }
        },
        "series": [
            {
                "name": "COUNT",
                "type": "bar",
                "data": [sentiment_counts['positive'],sentiment_counts['negative']],
                "itemStyle": {
                    "color": "#ef4136"
                }
            }
        ]
    }

    st_echarts(options=option, height="600px")
 

# --------------------------------------------------------------------------------------
st.write("---")
def load_and_process_data(file_path):
    # å®šç¾©ç†±é–€è‚¡ç¥¨ä»£ç¢¼èˆ‡å…¬å¸åç¨±ï¼ˆé€™éƒ¨åˆ†ä¿æŒä¸è®Šï¼‰
    popular_stocks = {
        "2618": "é•·æ¦®èˆª",
        "2883": "å‡±åŸºé‡‘",
        "9105": "æ³°é‡‘å¯¶-DR",
        "2303": "è¯é›»",
        "3706": "ç¥é”",
        "3481": "ç¾¤å‰µ",
        "3019": "äºå…‰",
        "00919": "ç¾¤ç›Šå°ç£ç²¾é¸é«˜æ¯",
        "2888": "æ–°å…‰é‡‘",
        "2887": "å°æ–°é‡‘",
        "2354": "é´»æº–",
        "2317": "é´»æµ·",
        "6770": "åŠ›ç©é›»",
        "2353": "å®ç¢",
        "3029": "é›¶å£¹",
        "2313": "è¯é€š",
        "2609": "é™½æ˜",
        "00965": "å…ƒå¤§èˆªå¤ªé˜²è¡›ç§‘æŠ€",
        "2344": "è¯é‚¦é›»",
        "3037": "æ¬£èˆˆ",
        "4540": "å…¨çƒå‚³å‹•",
        "2002": "ä¸­é‹¼",
        "2368": "é‡‘åƒé›»",
        "2890": "æ°¸è±é‡‘",
        "2312": "é‡‘å¯¶",
        "2886": "å…†è±é‡‘",
        "2308": "å°é”é›»",
        "00637L": "å…ƒå¤§æ»¬æ·±300æ­£2",
        "00939": "çµ±ä¸€å°ç£é«˜æ¯å‹•èƒ½",
        "1605": "è¯æ–°",
        "5876": "ä¸Šæµ·å•†éŠ€",
        "2409": "å‹é”",
        "00934": "ä¸­ä¿¡æˆé•·é«˜è‚¡æ¯",
        "3231": "ç·¯å‰µ",
        "3450": "è¯éˆ",
        "00945B": "å‡±åŸºç¾åœ‹éæŠ•ç­‰å‚µ",
        "2383": "å°å…‰é›»",
        "2891": "ä¸­ä¿¡é‡‘",
        "2880": "è¯å—é‡‘",
        "00918": "å¤§è¯å„ªåˆ©é«˜å¡«æ¯30",
        "2892": "ç¬¬ä¸€é‡‘",
        "4906": "æ­£æ–‡",
        "2359": "æ‰€ç¾…é–€",
        "2356": "è‹±æ¥­é”",
        "2392": "æ­£å´´",
        "2027": "å¤§æˆé‹¼",
        "2603": "é•·æ¦®",
        "1314": "ä¸­çŸ³åŒ–",
        "2399": "æ˜ æ³°",
        "00665L": "å¯Œé‚¦æ’ç”Ÿåœ‹ä¼æ­£2",
        "1101": "å°æ³¥",
        "2884": "ç‰å±±é‡‘",
        "1326": "å°åŒ–",
        "2606": "è£•æ°‘",
        "8070": "é•·è¯",
        "1402": "é æ±æ–°",
        "2455": "å…¨æ–°",
        "2382": "å»£é”",
        "6505": "å°å¡‘åŒ–",
        "00753L": "ä¸­ä¿¡ä¸­åœ‹50æ­£2",
        "8046": "å—é›»",
        "00929": "å¾©è¯å°ç£ç§‘æŠ€å„ªæ¯",
        "3032": "å‰è¨“",
        "6116": "å½©æ™¶",
        "00830": "åœ‹æ³°è²»åŸåŠå°é«”",
        "2371": "å¤§åŒ",
        "3014": "è¯é™½",
        "5880": "åˆåº«é‡‘",
        "1528": "æ©å¾·",
        "5871": "ä¸­ç§Ÿ-KY",
        "2454": "è¯ç™¼ç§‘",
        "2834": "è‡ºä¼éŠ€",
        "3045": "å°ç£å¤§",
        "00680L": "å…ƒå¤§ç¾å‚µ20æ­£2",
        "00715L": "æœŸè¡—å£å¸ƒè˜­ç‰¹æ­£2",
        "00940": "å…ƒå¤§å°ç£åƒ¹å€¼é«˜æ¯",
        "1808": "æ½¤éš†",
        "2324": "ä»å¯¶",
        "1504": "æ±å…ƒ",
        "2867": "ä¸‰å•†å£½",
        "3090": "æ—¥é›»è²¿",
        "3062": "å»ºæ¼¢",
        "9802": "éˆºé½Š-KY",
        "00915": "å‡±åŸºå„ªé¸é«˜è‚¡æ¯",
        "3661": "ä¸–èŠ¯-KY",
        "0056": "å…ƒå¤§é«˜è‚¡æ¯",
        "4977": "çœ¾é”-KY",
        "2301": "å…‰å¯¶ç§‘",
        "1514": "äºåŠ›",
        "00882": "ä¸­ä¿¡ä¸­åœ‹é«˜è‚¡æ¯",
        "1904": "æ­£éš†",
        "2637": "æ…§æ´‹-KY",
        "5284": "jpp-KY",
        "2412": "ä¸­è¯é›»",
        "3711": "æ—¥æœˆå…‰æŠ•æ§",
        "6005": "ç¾¤ç›Šè­‰",
        "00938": "å‡±åŸºå„ªé¸30",
        "1316": "ä¸Šæ›œ",
        "2889": "åœ‹ç¥¨é‡‘",
        "1513": "ä¸­èˆˆé›»"
    }
   # å‰µå»ºä¸€å€‹åå‘æ˜ å°„
    stock_name_to_code = {name: code for code, name in popular_stocks.items()}

    # åˆä½µæ‰€æœ‰è‚¡ç¥¨ä»£ç¢¼èˆ‡åç¨±ç‚ºå–®å€‹æ­£å‰‡è¡¨é”å¼
    all_patterns = "|".join([fr"\b{re.escape(code)}\b" for code in popular_stocks.keys()] +
                            [re.escape(name) for name in popular_stocks.values()])
    regex = re.compile(all_patterns)

    # è®€å– CSV æª”æ¡ˆ
    df = pd.read_csv(file_path)

    # æå–è‚¡ç¥¨å‡½æ•¸
    def extract_stocks(content, regex, stock_dict, name_to_code_dict):
        matches = regex.findall(str(content))
        if matches:
            unified_matches = set()
            for match in matches:
                if match in stock_dict:
                    unified_matches.add(stock_dict[match])
                elif match in name_to_code_dict:
                    unified_matches.add(match)
            return ", ".join(unified_matches) if unified_matches else ""
        return ""

    # åœ¨ content æ¬„ä½ä¸­é€²è¡Œæå–
    if "content" in df.columns:
        df["ç›¸é—œè‚¡ç¥¨"] = df["content"].apply(lambda x: extract_stocks(x, regex, popular_stocks, stock_name_to_code))

    # ç¯©é¸å‡ºåŒ…å«è‚¡ç¥¨çš„æ–‡ç« 
    result = df[df["ç›¸é—œè‚¡ç¥¨"] != ""]

    # çµ±è¨ˆè‚¡ç¥¨è¢«æåŠæ¬¡æ•¸
    stock_counts = result["ç›¸é—œè‚¡ç¥¨"].str.split(", ").explode().value_counts()

    # å°‡è‚¡ç¥¨ä»£ç¢¼æ›¿æ›å›è‚¡ç¥¨åç¨±
    stock_counts.index = stock_counts.index.map(lambda x: popular_stocks.get(x, x))

    return stock_counts
def main2():
    st.title("è‚¡ç¥¨è«–å£‡è¨è«–ç†±åº¦")
    st.caption("")
    # è®€å–ä¸¦è™•ç†è³‡æ–™
    file_path = "ptt_stock_filtered_content.csv"
    stock_counts = load_and_process_data(file_path)

    stock_counts_sorted = stock_counts.sort_values(ascending=True)
    
    # å°‡æ’åºå¾Œçš„çµæœæ”¾å…¥ DataFrameï¼Œä¸¦é‡æ–°æ’åºç´¢å¼•
    bottom_10_df = stock_counts_sorted.head(60).reset_index()
    bottom_10_df.columns = ['è‚¡ç¥¨', 'è¨è«–æ¬¡æ•¸']

    # é‡æ–°æ’åºå¾Œè¨­ç½®ç´¢å¼•ï¼Œä¿è­‰è¦–è¦ºåŒ–é †åºæ­£ç¢º
    bottom_10_df = bottom_10_df.sort_values(by='è¨è«–æ¬¡æ•¸', ascending=True)

    # ä½¿ç”¨ Streamlit çš„ bar_chart æ­£ç¢ºé¡¯ç¤º
    st.bar_chart(bottom_10_df.set_index('è‚¡ç¥¨')['è¨è«–æ¬¡æ•¸'])

main2()

st.title("å¸‚å ´æƒ…æ„Ÿåˆ¤æ–·")
sentiment_counts = {"positive": 0, "negative": 0}
with st.container():
    col1, col2 = st.columns(2)

    with col1:
        clicked1 = st.button("æ›´æ–°æ–‡ç« ", help="æ›´æ–°æœ€è¿‘æœ€å¤š 60 ç¯‡æœ‰æ•ˆæ–‡ç« ")
        if clicked1:
            len_post,deletedcontain=csv_content()
    
    with col2:
        clicked2 = st.button("æ›´æ–°æƒ…æ„Ÿçµ±è¨ˆ", help="æ›´æ–°æœ€è¿‘æœ€å¤š 60 ç¯‡æœ‰æ•ˆæ–‡ç« æƒ…æ„Ÿçµ±è¨ˆ")
        if clicked2:
            
            results=sen_ana(sentiment_counts)
            if sentiment_counts['positive']>sentiment_counts['negative']:
                st.balloons()
            else:
                st.snow()
                        #len_post,deletedcontain=csv_content()
    
colsuccess, colwarning = st.columns(2)
if clicked1:
    with colsuccess:
        st.success(f"è³‡æ–™å·²æˆåŠŸåŒ¯å‡ºé»æ“Šæ›´æ–°æƒ…æ„Ÿä»¥æ›´æ–°æœ‰æ•ˆæ–‡ç« æƒ…æ„Ÿçµ±è¨ˆï¼š{len_post}")


if clicked2:
    sentiment_df = pd.DataFrame(results)
    st.write("### Sentiment Counts:", sentiment_counts)
    positive_count = sentiment_counts['positive']
    negative_count = sentiment_counts['negative']
    display_bar_chart(sentiment_counts)
    sentiment_df = pd.DataFrame(results)
    st.markdown("### Results Table")
    st.write(sentiment_df)
# é›™å‘ LSTM ä»‹ç´¹
with st.expander("é›™å‘ LSTM (BiLSTM) ä»‹ç´¹"):
    st.markdown("""
    ## é›™å‘ LSTM (BiLSTM) ä»‹ç´¹

    é›™å‘ LSTMï¼ˆBidirectional Long Short-Term Memoryï¼‰æ˜¯ä¸€ç¨®ç‰¹åˆ¥çš„ LSTM æ¨¡å‹ï¼Œå®ƒåœ¨è™•ç†åºåˆ—æ•¸æ“šæ™‚ï¼Œå°‡è¼¸å…¥åºåˆ—åŒæ™‚å¾å…©å€‹æ–¹å‘é€²è¡Œè™•ç†â€”â€”æ­£å‘å’Œåå‘ã€‚é€™ä½¿å¾—æ¨¡å‹èƒ½å¤ åˆ©ç”¨æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¾è€Œæé«˜é æ¸¬æ€§èƒ½ã€‚

    ### BiLSTM çš„çµæ§‹

    èˆ‡å‚³çµ±çš„ LSTM æ¨¡å‹ç›¸æ¯”ï¼ŒBiLSTM æ“æœ‰å…©å±¤ LSTM çµ„ä»¶ï¼š
    1. **æ­£å‘ LSTM**ï¼šå¾åºåˆ—çš„é–‹å§‹è™•åˆ°çµæŸï¼Œé †åºåœ°è™•ç†æ•¸æ“šã€‚
    2. **åå‘ LSTM**ï¼šå¾åºåˆ—çš„çµæŸè™•å›åˆ°é–‹å§‹ï¼Œé€†å‘è™•ç†æ•¸æ“šã€‚

    é€™å…©å±¤ LSTM çš„è¼¸å‡ºæœƒè¢«çµåˆï¼ˆé€šå¸¸æ˜¯ä¸²æ¥æˆ–åŠ æ¬Šå¹³å‡ï¼‰ï¼Œå½¢æˆæœ€çµ‚çš„è¼¸å‡ºã€‚é€™æ¨£åšçš„ç›®çš„æ˜¯è®“æ¨¡å‹èƒ½å¤ åŒæ™‚è€ƒæ…®åºåˆ—çš„éå»å’Œæœªä¾†ä¿¡æ¯ã€‚

    ### BiLSTM çš„æ‡‰ç”¨

    BiLSTM ä¸»è¦æ‡‰ç”¨æ–¼éœ€è¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åºåˆ—è™•ç†ä»»å‹™ï¼š
    - **èªè¨€æ¨¡å‹**ï¼šèªè¨€ç†è§£ã€æƒ…æ„Ÿåˆ†æã€‚
    - **èªéŸ³è­˜åˆ¥**ï¼šå¯ä»¥è€ƒæ…®èªéŸ³çš„ä¸Šä¸‹æ–‡ã€‚
    - **æ©Ÿå™¨ç¿»è­¯**ï¼šè™•ç†æºèªè¨€å’Œç›®æ¨™èªè¨€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

    ### å„ªé»
    - èƒ½å¤ æ•æ‰æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆé›™å‘çš„å‰å¾Œé—œä¿‚ï¼‰ã€‚
    - å¢åŠ äº†æ¨¡å‹çš„è¡¨é”èƒ½åŠ›ï¼Œé©ç”¨æ–¼æ›´å¾©é›œçš„åºåˆ—æ•¸æ“šã€‚

    ### ç¼ºé»
    - è¨ˆç®—é‡å’Œå…§å­˜éœ€æ±‚è¼ƒé«˜ï¼Œå› ç‚ºæ¨¡å‹éœ€è¦è™•ç†å…©å€‹æ–¹å‘çš„åºåˆ—ã€‚
    """)

# æ³¨æ„åŠ›æ©Ÿåˆ¶ä»‹ç´¹
with st.expander("æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism) ä»‹ç´¹"):
    st.markdown("""
    ## æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism) ä»‹ç´¹

    æ³¨æ„åŠ›æ©Ÿåˆ¶æ˜¯ä¸€ç¨®æ¨¡ä»¿äººé¡è¦–è¦ºæ³¨æ„åŠ›çš„ç®—æ³•ï¼Œç”¨ä¾†ä½¿æ¨¡å‹èƒ½å¤ å°ˆæ³¨æ–¼åºåˆ—ä¸­çš„é—œéµéƒ¨åˆ†ã€‚å®ƒåœ¨è™•ç†é•·åºåˆ—æ™‚å°¤å…¶æœ‰ç”¨ï¼Œå› ç‚ºå®ƒå¯ä»¥å¹«åŠ©æ¨¡å‹â€œé¸æ“‡æ€§åœ°â€é—œæ³¨åºåˆ—ä¸­çš„é‡è¦ä½ç½®ï¼Œè€Œéå°æ•´å€‹åºåˆ—é€²è¡Œå¹³ç­‰çš„è™•ç†ã€‚

    ### æ³¨æ„åŠ›æ©Ÿåˆ¶çš„å·¥ä½œåŸç†

    æ³¨æ„åŠ›æ©Ÿåˆ¶æœƒæ ¹æ“šæŸå€‹è¼¸å…¥çš„â€œæŸ¥è©¢â€ä¾†è¨ˆç®—æ¯å€‹å…ƒç´ çš„æ¬Šé‡ï¼Œé€™äº›æ¬Šé‡æ±ºå®šäº†æ¨¡å‹æ‡‰è©²å°‡å¤šå°‘æ³¨æ„åŠ›é›†ä¸­åœ¨è©²å…ƒç´ ä¸Šã€‚é€šå¸¸ï¼Œé€™äº›æ¬Šé‡æ˜¯é€šéè¨ˆç®—æŸ¥è©¢èˆ‡æ‰€æœ‰éµçš„ç›¸ä¼¼åº¦ä¾†ç²å¾—çš„ã€‚

    - **æŸ¥è©¢ (Query)**ï¼šç”¨ä¾†æŸ¥æ‰¾åºåˆ—ä¸­ç›¸é—œä¿¡æ¯çš„å‘é‡ã€‚
    - **éµ (Key)**ï¼šåºåˆ—ä¸­çš„æ¯å€‹å…ƒç´ ï¼Œæ¨¡å‹ç”¨å®ƒä¾†æ±ºå®šæ˜¯å¦éœ€è¦é—œæ³¨è©²å…ƒç´ ã€‚
    - **å€¼ (Value)**ï¼šå°æ‡‰æ–¼éµçš„è¼¸å‡ºï¼Œç¶“éåŠ æ¬Šå¾Œè¢«é¸æ“‡æ€§åœ°ç”¨æ–¼æœ€çµ‚è¼¸å‡ºã€‚

    åœ¨è¨ˆç®—éç¨‹ä¸­ï¼Œé€šéå…§ç©æˆ–å…¶ä»–ç›¸ä¼¼åº¦æ¸¬é‡ä¾†è¨ˆç®—æŸ¥è©¢å’Œéµä¹‹é–“çš„ç›¸ä¼¼åº¦ï¼Œç„¶å¾Œæ ¹æ“šç›¸ä¼¼åº¦ç‚ºå€¼åˆ†é…æ¬Šé‡ã€‚

    ### æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ‡‰ç”¨

    - **æ©Ÿå™¨ç¿»è­¯**ï¼šæ³¨æ„åŠ›æ©Ÿåˆ¶å¯ä»¥è®“æ¨¡å‹åœ¨ç”Ÿæˆç¿»è­¯æ™‚å°ˆæ³¨æ–¼æºèªè¨€çš„é—œéµéƒ¨åˆ†ã€‚
    - **åœ–åƒæè¿°ç”Ÿæˆ**ï¼šåœ¨ç”Ÿæˆæè¿°æ™‚ï¼Œæ¨¡å‹å¯ä»¥å°ˆæ³¨æ–¼åœ–åƒä¸­çš„é‡è¦å€åŸŸã€‚
    - **èªéŸ³è­˜åˆ¥**ï¼šåœ¨èªéŸ³è½‰æ–‡å­—çš„éç¨‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥é¸æ“‡æ€§åœ°å°ˆæ³¨æ–¼ç‰¹å®šçš„æ™‚é–“æ­¥ã€‚

    ### å„ªé»
    - èƒ½å¤ è™•ç†é•·åºåˆ—æ•¸æ“šï¼Œå…‹æœäº†å‚³çµ± RNN å’Œ LSTM åœ¨é•·è·é›¢ä¾è³´è™•ç†ä¸Šçš„å±€é™æ€§ã€‚
    - å¢å¼·äº†æ¨¡å‹çš„å¯è§£é‡‹æ€§ï¼Œå¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°æ¨¡å‹é—œæ³¨çš„é—œéµéƒ¨åˆ†ã€‚

    ### ç¼ºé»
    - è¨ˆç®—é–‹éŠ·è¼ƒå¤§ï¼Œå°¤å…¶æ˜¯åœ¨é•·åºåˆ—çš„æƒ…æ³ä¸‹ã€‚
    - éœ€è¦æ›´å¤šçš„è¨ˆç®—è³‡æºï¼Œå°¤å…¶æ˜¯åœ¨å¤šå±¤æ³¨æ„åŠ›çµæ§‹ä¸­ã€‚
    """)

# --------------------------------------------------------------------------------------

st.write("---")
st.markdown("# ç®—æ³•è‚¡åƒ¹å»ºè­°åƒ¹æ ¼")
st.markdown("### æ·±åº¦å·ç©ç¶²è·¯(2D CNN)")

timestep = 10


# åŠ è½½Scaler
sc = joblib.load("sc.pkl")
sc_2 = joblib.load("sc_2.pkl")

ticker_all = ["2303.TW", "2330.TW", "2317.TW", "2412.TW"]
pre_all = [0] * len(ticker_all)
compare_dcnn_data = [0] * len(ticker_all)
open_diff = [0] * len(ticker_all)
open_pct = [0] * len(ticker_all)

end_date = datetime.today().strftime('%Y-%m-%d')
start_date = '2024-11-28'

for i, ticker_ever in enumerate(ticker_all):
    data = yf.download(ticker_ever, start=start_date, end=end_date)
    data.reset_index(inplace=True)

    data_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    data = data[data_columns].values.astype('float')

    compare_dcnn_data[i] = data[-1, 0]

    data = sc.transform(data)

    def prepare_prediction_data(data, timestep):
        input_data = []
        for j in range(len(data) - timestep):
            input_data.append(data[j:j + timestep])
        return np.array(input_data)
    
    input_data = prepare_prediction_data(data, timestep)
    input_tensor = torch.from_numpy(input_data).to(torch.float32)

    with torch.no_grad():
        predictions = model_dcnn(input_tensor.unsqueeze(1))  # æ·»åŠ é€šé“ç»´åº¦
    predictions = predictions.numpy()
    predictions_inverse = sc_2.inverse_transform(predictions)
    

    pre_all[i] = round(predictions_inverse[-1].item(), 1)
    open_diff[i] = pre_all[i]-compare_dcnn_data[i]
    open_pct[i] = (open_diff[i] / compare_dcnn_data[i]) * 100

with st.container():
    columns = st.columns(len(ticker_all))
    
    for i, ticker_ever in enumerate(ticker_all):
        columns[i].metric(
            label=ticker_ever,
            value=f"{pre_all[i]}",
            delta=f"{open_diff[i]:+.2f} ({open_pct[i]:+.2f}%)"
        )
with st.expander("é»æ“ŠæŸ¥çœ‹ 2D CNN ä»‹ç´¹"):
    st.markdown(cnn_intro)       
# -------------------------------
st.markdown("### é•·çŸ­æ•ˆç¥ç¶“ç¶²è·¯(LSTM)")    
def transform_data(df):
    data_index =  ['Open','High','Low','Close','Volume']
    flatten_data = df[data_index].values.reshape(-1)  # æ”¤å¹³è³‡æ–™
    str_data = "<SEP>".join(flatten_data.astype('str'))
    filter_data = str_data.replace(',',"").replace('X',"")
    x_data = filter_data.split("<SEP>")
    return x_data


for i, ticker_ever in enumerate(ticker_all):
    data = yf.download(ticker_ever, start=start_date, end=end_date)
    data.reset_index(inplace=True)

    data_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    data = data[data_columns].values.astype('float')

    compare_dcnn_data[i] = data[-1, 0]

    data = sc.transform(data)

    def prepare_prediction_data(data, timestep):
        input_data = []
        for j in range(len(data) - timestep):
            input_data.append(data[j:j + timestep])
        return np.array(input_data)
    
    input_data = prepare_prediction_data(data, timestep)
    input_tensor = torch.from_numpy(input_data).to(torch.float32)

    with torch.no_grad():
        predictions = model_1(input_tensor)  
    predictions = predictions.numpy()
    predictions_inverse = sc_2.inverse_transform(predictions)
    

    pre_all[i] = round(predictions_inverse[-1].item(), 1)
    open_diff[i] = pre_all[i]-compare_dcnn_data[i]
    open_pct[i] = (open_diff[i] / compare_dcnn_data[i]) * 100


with st.container():
    columns1 = st.columns(len(ticker_all))
    
    for i, ticker_ever in enumerate(ticker_all):
        columns1[i].metric(
            label=ticker_ever,
            value=f"{pre_all[i]}",
            delta=f"{open_diff[i]:+.2f} ({open_pct[i]:+.2f}%)"
        )

with st.expander("é»æ“ŠæŸ¥çœ‹ LSTM ä»‹ç´¹"):
    st.markdown(lstm_intro)
# --------------------------------------------------------------------------------------
